# Project 1
## General Info
Computer/CPU info can be found in the README at the root of this repository

## Usage
The default `make` target will create a build of each source file. `make clean`
can be run to delete all intermediate and output files generated by `make`.

The `generateIpcCsv.sh` script will run all of the compiled IPC programs (tests
1 and 2) ten times and output the results to a CSV file. From there,
`generateCharts.R` will read in the CSV data, print any required tables, and
create a pdf called `Rplots.pdf` containing any graphs.

To turn the PDF into a series of images I could embed in this markdown file, I
ran:
```
cpdf Rplots.pdf -gs gs -output-image -o images/chart%%.png
```
Of course, the ability to run some of these commands on your machine depends on
having the proper packages installed on your system, namely the 'R' language
(and some R libraries) and 'cpdf'/'gs' for pdf->png conversion.

## Features
For this, we were required to chose four features to experiment with. I have
chosen the following features:
1) Zero-copy I/O
2) Async I/O
3) Scheduler control groups
4) Transparent huge pages

## Tests 1 and 2: Zero-copy I/O and Async I/O
To test the effects of both zero-copy I/O and async I/O, I have decided to use
IPC between a parent and child process as my I/O. For a baseline, I setup a
simple named pipe and sent 10,000,000 integers down the pipe from the parent to
the child. Once I'd verified that all the numbers were coming through
consistently and there was no "packet loss," I added basic timing functionality.
Timing is started right before the parent process gets ready to send the first
number, and is stopped when the child process receives the final packet.

For the zero-copy I/O, a similar setup was used, but the named pipe was replaced
with a region of shared memory and a circular buffer. As the processes directly
shared the memory, there was no copying my the kernel in the IPC, and both
processes could read/write from the same buffer. Once again it was timed from
before the first number was sent, and after the last had been received.

For the async I/O, I went back to named pipes, but instead opened them in a
non-blocking fashion. As this program wasn't doing anything else, it couldn't
exactly make good use of the time saved by being non-blocking, and ultimately
just sat around waiting for each number to send anyways.

Each of these programs was run 10 times, and the average durations of each IPC
method across the 10 tests are listed in the table below.

|                   |   Baseline   |  Zero-copy  |    Async    |
|:-----------------:|:------------:|:-----------:|:-----------:|
| Mean Duration (s) |     11.4     |    2.91     |    25.4     |
|  Mean CPU Cycles  | 29647958444  | 7608492365  | 66317315250 |

As I had expected, the zero-copy I/O where the two processes directly share
memory performs significantly better than the baseline. The thing of note here,
however, is the fact that the async I/O took more than twice as long as the
baseline.

The way the blocking (baseline) reads/writes work is by simply waiting until the
FIFO is ready to receive a new letter or has one ready to be read. Once ready,
it reads or writes the data, and returns. By contrast, in the non-blocking mode,
we have to poll the FIFO manually to see if it's ready, and then only issue the
read/write command then. This almost certainly has more overhead than whatever
the kernel's doing internally on the blocking mode, and is my best reasoning for
why the async communication is so slow.

Of course, async I/O does have many uses. If this program did something other
than streaming data, it could take the time saved by not waiting on the blocking
reads/writes to perform other tasks, and only poll the FIFO when it's
convenient. Sure, the transmission may be slower, but it may make the overall
application faster, and the overhead has significantly less impact.
